---
title: Maximum likelihood
author: Rikard Wahlström
date: '2021-04-07'
slug: []
categories:
  - Statistics
tags:
  - statistics
  - R
Description: ''
Tags: []
Categories: []
DisableComments: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The lifespan (denoted as <span class="math inline">\(t\)</span>) of 100 lamps has been measured. Each lamp has been used with an intensity (denoted as <span class="math inline">\(s\)</span>) from 0 to 1, where 0 is turned off and 1 is maximum capacity.</p>
<p>It is known that lamps of the type relevant here has a lifespan <span class="math inline">\(t\)</span> that is exponentially distributed and where the expected value is</p>
<p><span class="math display">\[\mu(s) = \frac{\beta}{s}, \quad s &gt; 0,\]</span></p>
<p>where <span class="math inline">\(\beta &gt; 0\)</span> is an unknown parameter and <span class="math inline">\(s\)</span> equals the intensity with which the lamp has been used.</p>
</div>
<div id="assignments" class="section level1">
<h1>Assignments</h1>
<div id="assignment-1" class="section level2">
<h2>Assignment 1</h2>
<p><strong>Formulate the log-likelihood for <span class="math inline">\(\beta\)</span> given the observations</strong></p>
<p>We know that the lifespan of a lamp, <span class="math inline">\(t\)</span>, is exponentially distributed, that is:</p>
<p><span class="math display">\[f(t) = \lambda e^{-\lambda t}.\]</span></p>
<p>We know that the expected value of an exponentially distributed random variable (say <span class="math inline">\(t\)</span>) is given by:</p>
<p><span class="math inline">\(E[t] = \frac{1}{\lambda}.\)</span></p>
<p>We also know that, for the relevant sample, the expected value is given by the formula described in the introduction.</p>
<p>We then have that <span class="math inline">\(\frac{1}{\lambda} = \frac{\beta}{s}.\)</span></p>
<p>Solving for <span class="math inline">\(\lambda\)</span> we get that <span class="math inline">\(\lambda = \frac{s}{\beta}.\)</span></p>
<p>Thus, substituting <span class="math inline">\(\lambda\)</span> with <span class="math inline">\(\frac{s}{\beta}\)</span>, the likelihood function for one observation is given by <span class="math inline">\(L(\beta) = \frac{s}{\beta}e^{-\frac{s}{\beta}t}.\)</span></p>
<p>The likelihood function of the parameter <span class="math inline">\(\beta\)</span> given all observations is then given by:</p>
<p><span class="math display">\[\begin{aligned}
L_n(\beta) = \prod_{i = 1}^{n}\frac{s_i}{\beta}e^{-\frac{s_i}{\beta}t_i}.
\end{aligned}\]</span></p>
<p>And the log-likelihood:</p>
<p><span class="math display">\[\begin{aligned}
\ell_n(\beta) = 
lnL_n(\beta) = 
ln \prod_{i = 1}^{n}\frac{s_i}{\beta}e^{-\frac{s_i}{\beta}t_i} =
\sum_{i = 1}^n ln \left[ \frac{s_i}{\beta}e^{-\frac{s_i}{\beta}t_i} \right] = \\ 
\sum_{i = 1}^n \left[ ln s_i - ln \beta - \frac{1}{\beta}s_it_i \right] = 
\sum_{i = 1}^nln s_i - n ln \beta - \frac{1}{\beta}\sum_{i = 1}^n s_i t_i.
\end{aligned}\]</span></p>
</div>
<div id="assignment-2" class="section level2">
<h2>Assignment 2</h2>
<p><strong>Calculate the maximum likelihood estimate of <span class="math inline">\(\beta\)</span></strong></p>
<p>To calculate the maximum likelihood estimate of <span class="math inline">\(\beta\)</span>, we first take the derivative of <span class="math inline">\(\ell_n(\beta)\)</span> w.r.t. to <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\frac{d}{d\beta}\ell_n(\beta) = \ell_n&#39;(\beta) = 
-\frac{n}{\beta} + \frac{1}{\beta^2}\sum_{i = 1}^n s_i t_i.
\end{aligned}\]</span></p>
<p>Equating the resulting expression with <span class="math inline">\(0\)</span> and solving for <span class="math inline">\(\beta\)</span>, we get:</p>
<p><span class="math display">\[\begin{aligned}
-\frac{n}{\beta} + \frac{1}{\beta^2}\sum_{i = 1}^n s_i t_i = 0 \rightarrow
\frac{n}{\beta} = \frac{1}{\beta^2}\sum_{i = 1}^n s_i t_i \rightarrow
\hat \beta = \frac{1}{n}\sum_{i = 1}^n s_i t_i,
\end{aligned}\]</span></p>
<p>which is then the maximum likelihood estimate of <span class="math inline">\(\beta\)</span>.</p>
<p>The maximum likelihood estimate is therefore <span class="math inline">\(\hat \beta =\)</span> 121.425513.</p>
<p>We can double-check the calculation by calculating it both “by hand” using R and defining a function for finding the maximum likelihood estimate, as follows.</p>
<pre class="r"><code># Initial calculations

n &lt;- length(lamp_data$tid)

beta_hat &lt;- (1/n)*sum(lamp_data$styrka*lamp_data$tid)

# Formulate the log-likelihood function

log_ln &lt;- function(beta){
  n &lt;- length(lamp_data$tid)
  (sum(log(lamp_data$styrka))) - 
    ((n*log(beta))) -
    ((1/beta)*sum(lamp_data$tid*lamp_data$styrka))
}

# Calculating the maximum likelihood estimate

optim_result &lt;- optim(1.0,
                      log_ln,
                      method = &quot;Brent&quot;,
                      lower = 0.001,
                      upper = 50000,
                      control = list(fnscale = -1.0))

optim_result$par</code></pre>
<pre><code>## [1] 121.4255</code></pre>
<pre class="r"><code>beta_hat</code></pre>
<pre><code>## [1] 121.4255</code></pre>
<p>From the above, we see that the result is the same.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="assignment-3" class="section level2">
<h2>Assignment 3</h2>
<p><strong>Plot the regression line together with the observations to ensure that the estimate is reasonable</strong></p>
<p>Below is a plot of the observations, together with regression line (calculated from the expected value of <span class="math inline">\(t\)</span>, i.e. <span class="math inline">\(E[t] = \frac{\hat \beta}{s_i}\)</span>, for each observation):</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>From this, we can see that the ML estimate of <span class="math inline">\(\beta\)</span> seems reasonable.</p>
</div>
<div id="assignment-4" class="section level2">
<h2>Assignment 4</h2>
<p><strong>Test if <span class="math inline">\(\beta = 100\)</span> against <span class="math inline">\(\beta \neq 100\)</span>. Do it with Wilks, Wald and score tests</strong></p>
<div id="wilks-test" class="section level3">
<h3>Wilks’ test</h3>
<p>Wilks’ theorem is</p>
<p><span class="math display">\[
\lambda_{LR} = 2( l_n(\hat\beta)-l_n(\beta_o))\overset{asym.}{\sim} \chi_1^2.
\]</span>
Which we then calculate as follows:</p>
<pre class="r"><code># Calculate the statistic

beta_null &lt;- 100

wilks_statistic &lt;- 2*(log_ln(optim_result$par) - log_ln(beta_null))

wilks_statistic</code></pre>
<pre><code>## [1] 4.024861</code></pre>
<p>Which we then compare with the <span class="math inline">\(\chi_1^2\)</span> and calculate the p-value as follows:</p>
<pre class="r"><code># Find the p-value

wilks_p &lt;- 1 - pchisq(wilks_statistic, 1)

wilks_p</code></pre>
<pre><code>## [1] 0.04483432</code></pre>
<p>We see that the p-value is small, and that we can reject <span class="math inline">\(H_0:\)</span> <span class="math inline">\(\beta = 100\)</span> in favour of the alternative <span class="math inline">\(H_1:\)</span> <span class="math inline">\(\beta \neq 100\)</span> at the 5 percent significance level.</p>
</div>
<div id="walds-test" class="section level3">
<h3>Wald’s test</h3>
<p>For Wald’s test, we have the test statistic</p>
<p><span class="math display">\[
\frac{\hat\beta- \beta_0}{\textrm{Sd}(\hat\beta)} \overset{asym.}{\sim} \mathsf{N}(0,1),
\]</span></p>
<p>where the standard deviation can be calculated as</p>
<p><span class="math display">\[
Sd(\hat\beta) = (n\hat I(\hat\beta))^{-1/2} = \frac{1}{\sqrt{n\hat I(\hat\beta)}}.
\]</span></p>
<p>We therefore have to find the Fisher information <span class="math inline">\(I(\beta) = -E[\ell&#39;&#39;(\beta)]\)</span>.</p>
<p>Previously, we calculated</p>
<p><span class="math display">\[\begin{aligned}
\ell&#39;(\beta) = 
\frac{1}{\beta^2}st - \frac{1}{\beta}.
\end{aligned}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{aligned}
\ell&#39;&#39;(\beta) = 
\frac{1}{\beta^2} - \frac{2}{\beta^3}st.
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
I(\beta) = -E[\ell&#39;&#39;(\beta)] = -E[\frac{1}{\beta^2} - \frac{2}{\beta^3}st] = \\ -\frac{1}{\beta^2} + \frac{2}{\beta^3}sE[t] = 
-\frac{1}{\beta^2} + \frac{2}{\beta^3}s\frac{\beta}{s} = \\
-\frac{1}{\beta^2} + \frac{2\beta}{\beta^3} = 
-\frac{1}{\beta^2} + \frac{2}{\beta^2} = 
\frac{2-1}{\beta^2} = 
\frac{1}{\beta^2}.  
\end{aligned}\]</span></p>
<p>We then have that:</p>
<p><span class="math display">\[\begin{aligned}
nI(\beta) = n\frac{1}{\beta^2} = \frac{n}{\beta^2},
\end{aligned}\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[
Sd(\hat\beta) = \frac{1}{\sqrt{\frac{n}{\hat\beta^2}}} = \frac{\sqrt{\hat\beta^2}}{\sqrt{n}} = \frac{\hat\beta}{\sqrt{n}}.
\]</span></p>
<p>Using all of the, above, we can calculate the test statistic:</p>
<p><span class="math display">\[
\frac{\hat\beta- \beta_0}{\textrm{Sd}(\hat\beta)} = \frac{121.4255 - 100}{\frac{121.4255}{\sqrt{100}}} = 1.764498.
\]</span></p>
<p>We can calculate the p-value given the test statistic directly in R:</p>
<pre class="r"><code>2*pnorm(-abs(1.764498))</code></pre>
<pre><code>## [1] 0.07764817</code></pre>
<p>With the conclusion that, at the 5 percent significance level, we do not reject <span class="math inline">\(H_0\)</span>.</p>
<p>Implementing the whole process above in R, we arrive at a similar conclusion:</p>
<pre class="r"><code># Calculate the Fisher information

observedFisherInfo &lt;- function(beta){
  drop(-pracma::hessian(log_ln,beta))
}

observedFisherInfo(beta_hat)</code></pre>
<pre><code>## [1] 0.006774902</code></pre>
<pre class="r"><code># Calculate the test statistic

zWald &lt;- function(beta_null){
  abs(beta_hat - beta_null)*sqrt(observedFisherInfo(beta_hat))
}

# Find the p-value 

2 * ( 1 - pnorm( zWald(beta_null) ) )</code></pre>
<pre><code>## [1] 0.07781121</code></pre>
<p>We see that the p-value is close to that calculated “by hand” above (differences due to rounding) and that it is larger than <span class="math inline">\(\alpha = 0.05\)</span>, and thus, for the Wald test, we do not reject <span class="math inline">\(H_0\)</span>.</p>
</div>
<div id="score-test" class="section level3">
<h3>Score test</h3>
<p>For the Score test, we have the test statistic:</p>
<p><span class="math display">\[
\frac{l_n&#39;(\theta_0)}{\sqrt{I_n(\theta_0)}} \overset{asym.}{\sim} \mathsf N(0,1).
\]</span></p>
<p>We can calculate this, and the corresponding p-value, in a way similar to what was done for Wald’s test:</p>
<pre class="r"><code># Calculate the test statistic

zScore &lt;- function(beta_null){
  abs(pracma::grad(log_ln,beta_null)/sqrt(observedFisherInfo(beta_null)))
}

# Find the p-value

2 * ( 1 - pnorm(zScore(beta_null) ) )</code></pre>
<pre><code>## [1] 0.07292737</code></pre>
<p>Which, as for Wald’s test, leads us to not reject <span class="math inline">\(H_0\)</span>.</p>
</div>
</div>
<div id="assignment-5" class="section level2">
<h2>Assignment 5</h2>
<p><strong>Calculate a confidence interval for <span class="math inline">\(\beta\)</span> based on the Wald statistic</strong></p>
<p>As we saw above, Wald’s test is based on the fact that for large <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\frac{\hat\beta- \beta_0}{\textrm{Sd}(\hat\beta)} \overset{asym.}{\sim} \mathsf{N}(0,1).
\]</span>
We can then calculate a <span class="math inline">\(1-\alpha\)</span> percent confidence interval as follows:</p>
<p><span class="math display">\[
1-\alpha = 
P\left( -z_{\alpha/2} \leq \frac{\hat \beta - \beta}{Sd(\hat \beta)} \leq z_{\alpha/2}  \right) = 
P\left( \hat \beta - Sd(\hat \beta) z_{\alpha_2} \leq \beta \leq \hat \beta + Sd(\hat \beta)z_{\alpha/2} \right)
\]</span>
We can implement this in R as follows:</p>
<pre class="r"><code>alpha &lt;- 0.05

leftCILimit &lt;- beta_hat - qnorm(1-alpha/2) / sqrt(observedFisherInfo(beta_hat))

rightCILimit &lt;- beta_hat + qnorm(1-alpha/2) / sqrt(observedFisherInfo(beta_hat))

leftCILimit</code></pre>
<pre><code>## [1] 97.61347</code></pre>
<pre class="r"><code>rightCILimit</code></pre>
<pre><code>## [1] 145.2376</code></pre>
<p>Thus, we have that [97.6134748, 145.2375513] is a <span class="math inline">\(1-\alpha = 0.95\)</span> percent confidence interval for <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
